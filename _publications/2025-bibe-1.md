---
title: "On Semi-Autonomous, Intuitive, Lightmyography Based Control of Humanlike Robotic and Prosthetic Hands Utilizing Video and IMU Data"
collection: publications
category: conferences
pubtype: conference
tags: [teleoperation, shared-control, wearable-sensing, biosignal, manipulation, vision, perception, control]
doi: 10.1109/EMBC58623.2025.11252975
permalink: /publication/2025-bibe-1
excerpt: 'This paper introduces a vision-based teleoperation shared control framework designed to overcome real-time teleoperation limitations, providing intuitive, real-time control of a quadrupedâ€™s manipulator'
date: 2025-10-03
venue: '2025 IEEE International Conference on BioInformatics and BioEngineering (BIBE 2025)'
paperurl: 'https://ieeexplore.ieee.org/document/11252975'
citation: 'B. Guan, R. V. Godoy, A. Dwivedi and M. Liarokapis, "On the Impact of Different Light Wavelengths in Decoding Human Intention in Lightmyography Controlled Prosthetic Hands," 2025 47th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Copenhagen, Denmark, 2025, pp. 1-7, doi: 10.1109/EMBC58623.2025.11252975. keywords: {Hands;Computer vision;Machine learning algorithms;Light emitting diodes;Transformers;Skin;Decoding;Linear discriminant analysis;Convolutional neural networks;Random forests}'
---

Humanlike robotic hands such as prosthetic hands become more advanced as technology develops, giving us more lightweight, sophisticated solutions with multiple degrees of freedom. Alongside the hardware improvements, control systems and human machine interfaces are also important areas of research to ensure that the operation of robotic hands is intuitive and easy to master. In particular amputees, are frequently disappointed with the difficulty in controlling their prostheses, which can lead to prostheses rejection. One method that has been explored to reduce the effort and cognitive load on the user is to implement semi-autonomy via appropriate control schemes. In this paper, a semi-autonomous control framework is proposed employing lightmyography based decoding of grasping motions. The proposed framework makes use of video and IMU data so as to reduce the number of possible grasps (grasp affordances) based on the object detected and the hand orientation. The efficiency of the proposed framework has been experimentally validated in comparison to a manual control framework. Using the semi-autonomous framework, misclassifications decreased, leading to 17/20 successful reach to grasps motions executed compared to 7/20 for the manual control case for a single subject. The automatically positioned thumb functionality has also robustified grasping, allowing certain objects to be more dexterously interacted with. 

